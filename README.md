```markdown
# Adobe Hackathon: Roundâ€¯1Aâ€¯+â€¯1B Offline Pipeline

This repository implements a fully offline, CPUâ€‘only pipeline for:

1. **Roundâ€¯1A**: Extracting document structure (titleâ€¯+â€¯H1â€“H3 headings with page numbers) from PDFs via a local LLM (`allenai/unifiedqa-t5-small`).
2. **Roundâ€¯1B**: Personaâ€‘driven retrieval of the most relevant sections using a RAG approach (MiniLM + FAISS + TFâ€“IDF) and batch summarization (`sshleifer/distilbart-cnn-6-6`).

---

## ğŸ“‚ Repository Structure

```

adobe\_round1b/
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ round1a\_extractor.py
â”œâ”€â”€ preprocess.py
â”œâ”€â”€ embed\_index.py
â”œâ”€â”€ retrieve\_rank.py
â”œâ”€â”€ summarize.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ input/                  â† place your .pdf files here
â”‚   â””â”€â”€ \*.pdf
â””â”€â”€ output/                 â† generated JSON will appear here

````

---

## âš™ï¸ Prerequisites

- PythonÂ 3.8+  
- 2â€¯GB free disk (for model caches & indexes)  
- Windows/Linux/macOS terminal

---

## ğŸ“¥ Installation

1. **Create & activate** a virtual environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate   # or .\venv\Scripts\Activate.ps1 on Windows
````

2. **Install** dependencies:

   ```bash
   pip install -r requirements.txt
   ```

   `requirements.txt` includes:

   ```
   protobuf
   tiktoken
   sentencepiece
   PyMuPDF
   transformers>=4.33.2
   torch>=1.13.1
   sentence-transformers
   faiss-cpu
   scikit-learn
   scipy
   numpy
   ```

---

## â¬‡ï¸ Model Preâ€‘caching (ONEâ€‘TIME, ONLINE)

Before you run fully offline, **download** and cache the required models:

```bash
python - <<'EOF'
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from sentence_transformers import SentenceTransformer

# RoundÂ 1A extractor
AutoTokenizer.from_pretrained("allenai/unifiedqa-t5-small")
AutoModelForSeq2SeqLM.from_pretrained("allenai/unifiedqa-t5-small")

# RAG indexer
SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Summarizer
pipeline("summarization", model="sshleifer/distilbart-cnn-6-6")
EOF
```

This caches:

* `allenai/unifiedqa-t5-small`
* `sentence-transformers/all-MiniLM-L6-v2`
* `sshleifer/distilbart-cnn-6-6`

---

## ğŸš€ Running the Pipeline (OFFLINE)

Once cached, disable internet and run **endâ€‘toâ€‘end**:

```bash
python main.py \
  --persona "Travel Planner" \
  --job "Plan a trip of 4 days for a group of 10 college friends" \
  --input_dir ./input \
  --output_file ./output/challenge1b_output.json
```

* `--persona`: describes the target reader
* `--job`: describes the useâ€‘case/query
* `--input_dir`: folder with your `.pdf` files
* `--output_file`: path for the final JSON

---

## ğŸ“‹ Input & Output

* **Input**:

  * Place your PDFs in `input/`.
  * Roundâ€¯1A JSON outlines are autoâ€‘generated next to each PDF.

* **Output**:

  * Intermediate files:

    * `chunks.pkl`, `faiss.index`, `meta.json`, `tfidf.pkl`, `tfidf.npz`
  * **Final**: `output/challenge1b_output.json` containing:

    ```jsonc
    {
      "metadata": { "documents": [...], "persona": "...", "job": "...", "timestamp": "..." },
      "extracted_sections": [
        { "document":"...pdf", "page":2, "section_title":"...", "score":0.93 },
        â€¦
      ],
      "subsection_analysis": [
        { "doc_id":"...pdf", "page":2, "section_title":"â€¦", "refined_text":"â€¦" },
        â€¦
      ]
    }
    ```

---

## ğŸ”§ Module Summaries

* **`round1a_extractor.py`**
  Chunkâ€‘based LLM extraction of title + headings (local UnifiedQA).

* **`preprocess.py`**
  Reads the JSON outlines + PDFs â†’ tokenized text chunks.

* **`embed_index.py`**
  Builds a FAISS index of MiniLM embeddings + precomputes TFâ€“IDF.

* **`retrieve_rank.py`**
  Embeds the persona+job query â†’ retrieves topâ€‘K sections by hybrid cosine+TFâ€“IDF.

* **`summarize.py`**
  Batch summarizes top sections with DistilBART.

* **`main.py`**
  Orchestrates the full Roundâ€¯1Aâ†’1B pipeline.

---

## ğŸ›  Troubleshooting

* **â€œCannot findâ€¦cache and outgoing traffic disabledâ€**
  Ensure you ran the **Model Preâ€‘caching** step online.

* **Token limit warnings**
  All chunks are truncated to â‰¤â€¯200â€¯tokens; warnings can be ignored.

* **Empty `chunks.pkl`**
  Verify your Roundâ€¯1A JSONs contain nonâ€‘empty `headings`.

---

## ğŸ“„ License

This code is provided under the MIT License. SeeÂ [LICENSE](LICENSE) for details.

```
```
